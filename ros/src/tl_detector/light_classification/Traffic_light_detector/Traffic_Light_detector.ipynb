{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Capstone project\n",
    "\n",
    "### Project8: Traffic Light Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Grab test images\n",
    "img_dir = \"classification_images\" # Enter Directory of all images \n",
    "data_path = os.path.join(img_dir,'*g')\n",
    "images = glob.glob(data_path)\n",
    "classification_images = []\n",
    "test_images = []\n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images:\n",
    "    classification_images.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model    \n",
    "for img in classification_images:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Model path\n",
    "model_path = \"ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb\"\n",
    "\n",
    "# Load the Tensorflow model into memory. \n",
    "detection_graph = tf.Graph() \n",
    "with detection_graph.as_default(): \n",
    "    od_graph_def = tf.GraphDef() \n",
    "    with tf.gfile.GFile(model_path, 'rb') as fid: \n",
    "        serialized_graph = fid.read() \n",
    "        od_graph_def.ParseFromString(serialized_graph) \n",
    "        tf.import_graph_def(od_graph_def, name ='') \n",
    "  \n",
    "    sess = tf.Session(graph = detection_graph) \n",
    "    \n",
    "print(len(classification_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output tensors (i.e. data) for the object detection classifier \n",
    "  \n",
    "# Input tensor is the image \n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0') \n",
    "  \n",
    "# Output tensors are the detection boxes, scores, and classes \n",
    "# Each box represents a part of the image where a particular object was detected \n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0') \n",
    "  \n",
    "# Each score represents level of confidence for each of the objects. \n",
    "# The score is shown on the result image, together with the class label. \n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0') \n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0') \n",
    "  \n",
    "# Number of objects detected \n",
    "num_detections = detection_graph.get_tensor_by_name('num_detections:0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the actual detection by running the model with the image as input \n",
    "images_with_boxes = []\n",
    "\n",
    "# Detect traffic light boxes, scores for detection and classes\n",
    "def detection(expand_image): \n",
    "    (boxes, scores, classes, num) = sess.run( \n",
    "        [detection_boxes, detection_scores, detection_classes, num_detections], \n",
    "        feed_dict ={image_tensor: expand_image})\n",
    "    return boxes, scores, classes\n",
    "\n",
    "# Draw boxes on image\n",
    "def draw_boxes(image, boxes, scores, classes):    \n",
    "    for parameter in zip(boxes[0], classes[0], scores[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * image.shape[0])\n",
    "            x_max = int(box[2] * image.shape[0])\n",
    "            y_min = int(box[1] * image.shape[1])\n",
    "            y_max = int(box[3] * image.shape[1])\n",
    "            image = cv2.rectangle(image, (y_min,x_min),(y_max,x_max), (0,255,0), 5)\n",
    "    images_with_boxes = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = []\n",
    "scores = []\n",
    "classes = []\n",
    "\n",
    "for i in range (len(classification_images)):\n",
    "    boxes, scores, classes = detection(test_images[i])\n",
    "    images_with_boxes.append(draw_boxes(classification_images[i], boxes, scores, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = yaml.load(open(\"data_train/train.yaml\"))\n",
    "labels = []\n",
    "train_images = []\n",
    "\n",
    "# Cut the image looking at the boxes in the yaml\n",
    "def cut_boxes(train_data):\n",
    "    for train_datum in train_data:\n",
    "        if len(train_datum['boxes']) != 0:\n",
    "            image_path = train_datum['path']\n",
    "            image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_RGB2BGR)\n",
    "            for box in train_datum['boxes']:\n",
    "                label = box['label']\n",
    "                x_min = int(box['x_min'])\n",
    "                x_max = int(box['x_max'])\n",
    "                y_min = int(box['y_min'])\n",
    "                y_max = int(box['y_max'])\n",
    "                if (x_max - x_min) > 10:\n",
    "                    train_image = image[y_min:y_max,x_min:x_max,:]\n",
    "                    if train_image.shape[0] != 0:\n",
    "                        try:\n",
    "                            train_image = cv2.resize(train_image,(14,32))\n",
    "                            train_images.append(train_image)\n",
    "                            labels.append(label)\n",
    "                            len(labels)\n",
    "                        except:\n",
    "                            pass                        \n",
    "cut_boxes(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43eabd6f60>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[6])\n",
    "plt.savefig(\"train_images.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation data\n",
    "\n",
    "# Red \n",
    "red_path = \"data_train_sim/tl_data_red\" # Enter Directory of all images \n",
    "data_red_sim_path = os.path.join(red_path,'*g')\n",
    "\n",
    "images_sim_red = glob.glob(data_red_sim_path)\n",
    "images_sim_red.sort()\n",
    "\n",
    "# Green \n",
    "green_path = \"data_train_sim/tl_data_green\" # Enter Directory of all images \n",
    "data_green_sim_path = os.path.join(green_path,'*g')\n",
    "\n",
    "images_sim_green = glob.glob(data_green_sim_path)\n",
    "images_sim_green.sort()\n",
    "\n",
    "# Yellow \n",
    "yellow_path = \"data_train_sim/tl_data_yellow\" # Enter Directory of all images \n",
    "data_yellow_sim_path = os.path.join(yellow_path,'*g')\n",
    "\n",
    "images_sim_yellow = glob.glob(data_yellow_sim_path)\n",
    "images_sim_yellow.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_sim/tl_data_red/traffic_lights_data_red_001.jpg\n",
      "data_train_sim/tl_data_green/traffic_lights_data_green_001.jpg\n",
      "data_train_sim/tl_data_yellow/traffic_lights_data_yellow_001.jpg\n"
     ]
    }
   ],
   "source": [
    "print(images_sim_red[0])\n",
    "print(images_sim_green[0])\n",
    "print(images_sim_yellow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation image labeling\n",
    "labels_sim = []\n",
    "image_sim_paths = []\n",
    "\n",
    "for image in images_sim_red:\n",
    "    labels_sim.append('R')\n",
    "    image_sim_paths.append(image)\n",
    "for image in images_sim_green:\n",
    "    labels_sim.append('G')\n",
    "    image_sim_paths.append(image)\n",
    "for image in images_sim_yellow:\n",
    "    labels_sim.append('Y')\n",
    "    image_sim_paths.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle simlation data (images and labels) \n",
    "# data_list = list(zip(labels_sim, image_sim_paths))\n",
    "# random.shuffle(data_list)\n",
    "# labels_sim, image_sim_paths = zip(*data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform traffic light detection on simulation data\n",
    "classification_images_red = []\n",
    "test_images_red = []\n",
    "classification_images_green = []\n",
    "test_images_green = []\n",
    "classification_images_yellow = []\n",
    "test_images_yellow = []\n",
    "\n",
    "boxes_sim = []\n",
    "classes_sim = []\n",
    "scores_sim = []\n",
    "\n",
    "sim_images_box = []\n",
    "labels_sim = []\n",
    "  \n",
    "# RED CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_sim_red:\n",
    "    classification_images_red.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_red:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_red.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_sim_red)):\n",
    "# for i in range(1):\n",
    "    boxes_sim, scores_sim, classes_sim = detection(test_images_red[i])\n",
    "    for parameter in zip(boxes_sim[0], classes_sim[0], scores_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_red[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_red[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_red[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_red[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_red[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        sim_image_box = cv2.resize(image,(14,32))\n",
    "                        sim_image_box = cv2.cvtColor(sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        sim_images_box.append(sim_image_box)\n",
    "                        labels_sim.append('R')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# GREEN CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_sim_green:\n",
    "    classification_images_green.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_green:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_green.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_sim_green)):\n",
    "# for i in range(1):\n",
    "    boxes_sim, scores_sim, classes_sim = detection(test_images_green[i])\n",
    "    for parameter in zip(boxes_sim[0], classes_sim[0], scores_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_green[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_green[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_green[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_green[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_green[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        sim_image_box = cv2.resize(image,(14,32))\n",
    "                        sim_image_box = cv2.cvtColor(sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        sim_images_box.append(sim_image_box)\n",
    "                        labels_sim.append('G')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# YELLOW CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_sim_yellow:\n",
    "    classification_images_yellow.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_yellow:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_yellow.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_sim_yellow)):\n",
    "# for i in range(1):\n",
    "    boxes_sim, scores_sim, classes_sim = detection(test_images_yellow[i])\n",
    "    for parameter in zip(boxes_sim[0], classes_sim[0], scores_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_yellow[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_yellow[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_yellow[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_yellow[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_yellow[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        sim_image_box = cv2.resize(image,(14,32))\n",
    "                        sim_image_box = cv2.cvtColor(sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        sim_images_box.append(sim_image_box)\n",
    "                        labels_sim.append('Y')\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791\n",
      "791\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43eabd66a0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(sim_images_box))\n",
    "print(len(labels_sim))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sim_images_box[0])\n",
    "plt.savefig(\"sim_images_box0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label refactor: squeexe classes into the 4 we are interested in \n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'Yellow':\n",
    "        labels[i] = 'Y'\n",
    "    elif labels[i] == 'Red':\n",
    "        labels[i] = 'R'\n",
    "    elif labels[i] == 'RedLeft':\n",
    "        labels[i] = 'R'\n",
    "    elif labels[i] == 'RedRight':\n",
    "        labels[i] = 'R'\n",
    "    elif labels[i] == 'Green':\n",
    "        labels[i] = 'G'\n",
    "    elif labels[i] == 'GreenLeft':\n",
    "        labels[i] = 'G'\n",
    "    elif labels[i] == 'GreenRight':\n",
    "        labels[i] = 'G'\n",
    "    else:\n",
    "        labels[i] = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dictionary with dict() method \n",
    "label_dict = dict({'R' : 0, 'Y' : 1, 'G' : 2, 'O' : 3}) \n",
    "\n",
    "# Train lists\n",
    "x_real = []\n",
    "y_real = []\n",
    "\n",
    "# Use dict to store labels\n",
    "y_real = [label_dict[i] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform them for the model\n",
    "x_real = np.array(train_images)\n",
    "y_real = np.array(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 32, 14, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 12, 8)         224       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 10, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 28, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 8, 32)         4640      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 26, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 6, 64)         18496     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               921700    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 948,332\n",
      "Trainable params: 948,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3244 samples, validate on 812 samples\n",
      "Epoch 1/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.4384 - acc: 0.8471 - val_loss: 0.1473 - val_acc: 0.9631\n",
      "Epoch 2/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.2083 - acc: 0.9340 - val_loss: 0.1451 - val_acc: 0.9557\n",
      "Epoch 3/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1827 - acc: 0.9405 - val_loss: 0.1458 - val_acc: 0.9594\n",
      "Epoch 4/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1528 - acc: 0.9498 - val_loss: 0.1423 - val_acc: 0.9667\n",
      "Epoch 5/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1407 - acc: 0.9556 - val_loss: 0.1595 - val_acc: 0.9643\n",
      "Epoch 6/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1340 - acc: 0.9556 - val_loss: 0.1127 - val_acc: 0.9729\n",
      "Epoch 7/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1345 - acc: 0.9578 - val_loss: 0.1416 - val_acc: 0.9680\n",
      "Epoch 8/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1312 - acc: 0.9612 - val_loss: 0.1266 - val_acc: 0.9655\n",
      "Epoch 9/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1219 - acc: 0.9621 - val_loss: 0.1241 - val_acc: 0.9680\n",
      "Epoch 10/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1252 - acc: 0.9609 - val_loss: 0.1261 - val_acc: 0.9766\n",
      "Epoch 11/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1132 - acc: 0.9670 - val_loss: 0.1499 - val_acc: 0.9680\n",
      "Epoch 12/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1208 - acc: 0.9639 - val_loss: 0.1299 - val_acc: 0.9704\n",
      "Epoch 13/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1191 - acc: 0.9618 - val_loss: 0.1099 - val_acc: 0.9754\n",
      "Epoch 14/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1007 - acc: 0.9658 - val_loss: 0.1069 - val_acc: 0.9754\n",
      "Epoch 15/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.0949 - acc: 0.9695 - val_loss: 0.1144 - val_acc: 0.9717\n",
      "Epoch 16/16\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.1175 - acc: 0.9593 - val_loss: 0.0959 - val_acc: 0.9791\n"
     ]
    }
   ],
   "source": [
    "# Train with real data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(32,14,3)))\n",
    "model.add(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3),strides=(1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3) ,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_real,y_real,validation_split=0.2,shuffle=True,nb_epoch=16)\n",
    "model.save('model_real.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sim = []\n",
    "y_sim = []\n",
    "\n",
    "# Use dict to store labels\n",
    "y_sim= [label_dict[i] for i in labels_sim]\n",
    "\n",
    "# Transform them for the model\n",
    "x_sim = np.array(sim_images_box)\n",
    "y_sim = np.array(y_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "791/791 [==============================] - 0s 263us/step\n",
      "test loss, test acc: [0.22132795231532207, 0.96839443742098608]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_sim, y_sim, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 32, 14, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 12, 8)         224       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 10, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 8, 32)         4640      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 26, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 6, 64)         18496     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               921700    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 948,332\n",
      "Trainable params: 948,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 632 samples, validate on 159 samples\n",
      "Epoch 1/16\n",
      "632/632 [==============================] - 1s 2ms/step - loss: 0.2890 - acc: 0.8797 - val_loss: 12.6715 - val_acc: 0.2138\n",
      "Epoch 2/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0503 - acc: 0.9778 - val_loss: 12.6715 - val_acc: 0.2138\n",
      "Epoch 3/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0457 - acc: 0.9778 - val_loss: 12.6539 - val_acc: 0.2138\n",
      "Epoch 4/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0289 - acc: 0.9873 - val_loss: 9.1317 - val_acc: 0.2138\n",
      "Epoch 5/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0256 - acc: 0.9858 - val_loss: 11.8235 - val_acc: 0.2138\n",
      "Epoch 6/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0276 - acc: 0.9889 - val_loss: 10.6790 - val_acc: 0.2138\n",
      "Epoch 7/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0326 - acc: 0.9858 - val_loss: 8.3330 - val_acc: 0.2138\n",
      "Epoch 8/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0298 - acc: 0.9873 - val_loss: 9.0736 - val_acc: 0.2138\n",
      "Epoch 9/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0225 - acc: 0.9921 - val_loss: 12.6612 - val_acc: 0.2138\n",
      "Epoch 10/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0239 - acc: 0.9921 - val_loss: 12.3801 - val_acc: 0.2138\n",
      "Epoch 11/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0218 - acc: 0.9953 - val_loss: 12.6598 - val_acc: 0.2138\n",
      "Epoch 12/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0215 - acc: 0.9937 - val_loss: 12.0038 - val_acc: 0.2138\n",
      "Epoch 13/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0202 - acc: 0.9953 - val_loss: 12.6715 - val_acc: 0.2138\n",
      "Epoch 14/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0223 - acc: 0.9953 - val_loss: 12.6715 - val_acc: 0.2138\n",
      "Epoch 15/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0185 - acc: 0.9953 - val_loss: 12.4873 - val_acc: 0.2138\n",
      "Epoch 16/16\n",
      "632/632 [==============================] - 1s 1ms/step - loss: 0.0178 - acc: 0.9953 - val_loss: 12.6715 - val_acc: 0.2138\n"
     ]
    }
   ],
   "source": [
    "# Train with simulation data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(32,14,3)))\n",
    "model.add(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3),strides=(1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3) ,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_sim,y_sim,validation_split=0.2,shuffle=True,nb_epoch=16)\n",
    "model.save('model_sim.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "4056/4056 [==============================] - 1s 230us/step\n",
      "test loss, test acc: [0.64192731610886911, 0.83653846153846156]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_real, y_real, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create miscellaneous data set\n",
    "x_data_set = []\n",
    "y_data_set = []\n",
    "\n",
    "x_data_set = np.concatenate((x_real, x_sim))\n",
    "y_data_set = np.concatenate((y_real, y_sim))\n",
    "\n",
    "x_data_set, y_data_set = shuffle(x_data_set, y_data_set)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# Create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data_set, y_data_set, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_3 (Lambda)            (None, 32, 14, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 30, 12, 8)         224       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 10, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 28, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 26, 8, 32)         4640      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 26, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 6, 64)         18496     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               921700    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 948,332\n",
      "Trainable params: 948,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3101 samples, validate on 776 samples\n",
      "Epoch 1/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.4163 - acc: 0.8565 - val_loss: 0.2228 - val_acc: 0.9420\n",
      "Epoch 2/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.2011 - acc: 0.9361 - val_loss: 0.1868 - val_acc: 0.9446\n",
      "Epoch 3/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1795 - acc: 0.9378 - val_loss: 0.1740 - val_acc: 0.9510\n",
      "Epoch 4/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1608 - acc: 0.9465 - val_loss: 0.1832 - val_acc: 0.9536\n",
      "Epoch 5/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1543 - acc: 0.9529 - val_loss: 0.1571 - val_acc: 0.9549\n",
      "Epoch 6/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1553 - acc: 0.9526 - val_loss: 0.1725 - val_acc: 0.9510\n",
      "Epoch 7/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1364 - acc: 0.9568 - val_loss: 0.1716 - val_acc: 0.9562\n",
      "Epoch 8/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1416 - acc: 0.9507 - val_loss: 0.1590 - val_acc: 0.9588\n",
      "Epoch 9/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1295 - acc: 0.9607 - val_loss: 0.1400 - val_acc: 0.9639\n",
      "Epoch 10/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1375 - acc: 0.9597 - val_loss: 0.1440 - val_acc: 0.9613\n",
      "Epoch 11/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1257 - acc: 0.9600 - val_loss: 0.1418 - val_acc: 0.9562\n",
      "Epoch 12/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1162 - acc: 0.9616 - val_loss: 0.1473 - val_acc: 0.9601\n",
      "Epoch 13/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1178 - acc: 0.9636 - val_loss: 0.1667 - val_acc: 0.9536\n",
      "Epoch 14/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1199 - acc: 0.9623 - val_loss: 0.1482 - val_acc: 0.9626\n",
      "Epoch 15/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.1043 - acc: 0.9665 - val_loss: 0.1396 - val_acc: 0.9652\n",
      "Epoch 16/16\n",
      "3101/3101 [==============================] - 4s 1ms/step - loss: 0.0965 - acc: 0.9684 - val_loss: 0.1596 - val_acc: 0.9575\n"
     ]
    }
   ],
   "source": [
    "# Train with miscellaneous data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(32,14,3)))\n",
    "model.add(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3),strides=(1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3) ,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,validation_split=0.2,shuffle=True,nb_epoch=16)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "970/970 [==============================] - 0s 258us/step\n",
      "test loss, test acc: [0.078347467838488907, 0.97628865979381441]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# load the model we saved\n",
    "model = load_model('model.h5')\n",
    "classification = model.predict_classes(x_train[0].reshape(1, 32, 14, 3), batch_size=10)\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43ea831ac8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.savefig(\"x_train0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform traffic light detection on udacity record data\n",
    "classification_images_red = []\n",
    "test_images_red = []\n",
    "classification_images_green = []\n",
    "test_images_green = []\n",
    "classification_images_yellow = []\n",
    "test_images_yellow = []\n",
    "classification_images_none = []\n",
    "test_images_none = []\n",
    "\n",
    "boxes_record = []\n",
    "classes_record = []\n",
    "scores_record = []\n",
    "\n",
    "record_images_box = []\n",
    "labels_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load record data\n",
    "\n",
    "# Red \n",
    "red_path = \"data_train_record/tl_data_red\" # Enter Directory of all images \n",
    "data_red_record_path = os.path.join(red_path,'*g')\n",
    "\n",
    "images_record_red = glob.glob(data_red_record_path)\n",
    "images_record_red.sort()\n",
    "\n",
    "# Green \n",
    "green_path = \"data_train_record/tl_data_green\" # Enter Directory of all images \n",
    "data_green_record_path = os.path.join(green_path,'*g')\n",
    "\n",
    "images_record_green = glob.glob(data_green_record_path)\n",
    "images_record_green.sort()\n",
    "\n",
    "# Yellow \n",
    "yellow_path = \"data_train_record/tl_data_yellow\" # Enter Directory of all images \n",
    "data_yellow_record_path = os.path.join(yellow_path,'*g')\n",
    "\n",
    "images_record_yellow = glob.glob(data_yellow_record_path)\n",
    "images_record_yellow.sort()\n",
    "\n",
    "#None\n",
    "\n",
    "none_path = \"data_train_record/tl_data_none\" # Enter Directory of all images \n",
    "data_none_record_path = os.path.join(none_path,'*g')\n",
    "\n",
    "images_record_none = glob.glob(data_none_record_path)\n",
    "images_record_none.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RED CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_record_red:\n",
    "    classification_images_red.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_red:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_red.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_record_red)):\n",
    "# for i in range(1):\n",
    "    boxes_record, scores_record, classes_record = detection(test_images_red[i])\n",
    "    for parameter in zip(boxes_record[0], classes_record[0], scores_record[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_red[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_red[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_red[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_red[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_red[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        record_image_box = cv2.resize(image,(14,32))\n",
    "                        record_image_box = cv2.cvtColor(record_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        record_images_box.append(record_image_box)\n",
    "                        labels_record.append('R')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# GREEN CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_record_green:\n",
    "    classification_images_green.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_green:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_green.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_record_green)):\n",
    "# for i in range(1):\n",
    "    boxes_record, scores_record, classes_record = detection(test_images_green[i])\n",
    "    for parameter in zip(boxes_record[0], classes_record[0], scores_record[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_green[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_green[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_green[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_green[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_green[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        record_image_box = cv2.resize(image,(14,32))\n",
    "                        record_image_box = cv2.cvtColor(record_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        record_images_box.append(record_image_box)\n",
    "                        labels_record.append('G')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# YELLOW CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_record_yellow:\n",
    "    classification_images_yellow.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_yellow:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_yellow.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_record_yellow)):\n",
    "# for i in range(1):\n",
    "    boxes_record, scores_record, classes_record = detection(test_images_yellow[i])\n",
    "    for parameter in zip(boxes_record[0], classes_record[0], scores_record[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_yellow[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_yellow[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_yellow[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_yellow[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_yellow[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        record_image_box = cv2.resize(image,(14,32))\n",
    "                        record_image_box = cv2.cvtColor(record_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        record_images_box.append(record_image_box)\n",
    "                        labels_record.append('Y')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# NONE CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_record_none:\n",
    "    classification_images_none.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_none:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_none.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "# for i in range (len(image_record_none)):\n",
    "for i in range(1):\n",
    "    boxes_record, scores_record, classes_record = detection(test_images_none[i])\n",
    "    for parameter in zip(boxes_record[0], classes_record[0], scores_record[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_none[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_none[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_none[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_none[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_none[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        record_image_box = cv2.resize(image,(14,32))\n",
    "                        record_image_box = cv2.cvtColor(record_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        record_images_box.append(record_image_box)\n",
    "                        labels_record.append('O')\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_record = []\n",
    "y_record = []\n",
    "\n",
    "# Use dict to store labels\n",
    "y_record = [label_dict[i] for i in labels_record]\n",
    "\n",
    "# Transform them for the model\n",
    "x_record = np.array(record_images_box)\n",
    "y_record = np.array(y_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on record data\n",
      "345/345 [==============================] - 0s 471us/step\n",
      "test loss, test acc: [0.071639002319695291, 0.98260869772537895]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on record data')\n",
    "results = model.evaluate(x_record, y_record, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform traffic light detection on new sim data\n",
    "classification_images_red = []\n",
    "test_images_red = []\n",
    "classification_images_green = []\n",
    "test_images_green = []\n",
    "classification_images_yellow = []\n",
    "test_images_yellow = []\n",
    "classification_images_none = []\n",
    "test_images_none = []\n",
    "\n",
    "boxes_new_sim = []\n",
    "classes_new_sim = []\n",
    "scores_new_sim = []\n",
    "\n",
    "new_sim_images_box = []\n",
    "labels_new_sim = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new_sim data\n",
    "\n",
    "# Red \n",
    "\n",
    "red_path = \"data_train_new_sim/red\" # Enter Directory of all images \n",
    "data_red_new_sim_path = os.path.join(red_path,'*g')\n",
    "\n",
    "images_new_sim_red = glob.glob(data_red_new_sim_path)\n",
    "images_new_sim_red.sort()\n",
    "\n",
    "# Green \n",
    "green_path = \"data_train_new_sim/green\" # Enter Directory of all images \n",
    "data_green_new_sim_path = os.path.join(green_path,'*g')\n",
    "\n",
    "images_new_sim_green = glob.glob(data_green_new_sim_path)\n",
    "images_new_sim_green.sort()\n",
    "\n",
    "# Yellow \n",
    "yellow_path = \"data_train_new_sim/yellow\" # Enter Directory of all images \n",
    "data_yellow_new_sim_path = os.path.join(yellow_path,'*g')\n",
    "\n",
    "images_new_sim_yellow = glob.glob(data_yellow_new_sim_path)\n",
    "images_new_sim_yellow.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RED CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_new_sim_red:\n",
    "    classification_images_red.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_red:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_red.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_new_sim_red)):\n",
    "# for i in range(1):\n",
    "    boxes_new_sim, scores_new_sim, classes_new_sim = detection(test_images_red[i])\n",
    "    for parameter in zip(boxes_new_sim[0], classes_new_sim[0], scores_new_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_red[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_red[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_red[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_red[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_red[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        new_sim_image_box = cv2.resize(image,(14,32))\n",
    "                        new_sim_image_box = cv2.cvtColor(new_sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        new_sim_images_box.append(new_sim_image_box)\n",
    "                        labels_new_sim.append('R')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# GREEN CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_new_sim_green:\n",
    "    classification_images_green.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_green:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_green.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_new_sim_green)):\n",
    "# for i in range(1):\n",
    "    boxes_new_sim, scores_new_sim, classes_new_sim = detection(test_images_green[i])\n",
    "    for parameter in zip(boxes_new_sim[0], classes_new_sim[0], scores_new_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_green[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_green[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_green[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_green[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_green[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        new_sim_image_box = cv2.resize(image,(14,32))\n",
    "                        new_sim_image_box = cv2.cvtColor(new_sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        new_sim_images_box.append(new_sim_image_box)\n",
    "                        labels_new_sim.append('G')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "# YELLOW CASE    \n",
    "\n",
    "# Create the list of images to be classified\n",
    "for image in images_new_sim_yellow:\n",
    "    classification_images_yellow.append(cv2.imread(image))\n",
    "    \n",
    "# Squeeze them in 1-D and convert them for the model     \n",
    "for img in classification_images_yellow:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    test_images_yellow.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "# Cut the image looking at boxes detected\n",
    "for i in range (len(images_new_sim_yellow)):\n",
    "# for i in range(1):\n",
    "    boxes_new_sim, scores_new_sim, classes_new_sim = detection(test_images_yellow[i])\n",
    "    for parameter in zip(boxes_new_sim[0], classes_new_sim[0], scores_new_sim[0]):\n",
    "         if parameter[1] == 10 and parameter[2] >= .5:\n",
    "            box = parameter[0]\n",
    "            x_min = int(box[0] * classification_images_yellow[i].shape[0])\n",
    "            x_max = int(box[2] * classification_images_yellow[i].shape[0])\n",
    "            y_min = int(box[1] * classification_images_yellow[i].shape[1])\n",
    "            y_max = int(box[3] * classification_images_yellow[i].shape[1])\n",
    "            if (x_max - x_min) > 20:\n",
    "                image = classification_images_yellow[i][x_min:x_max,y_min:y_max,:]\n",
    "                if image.shape[0] != 0:\n",
    "                    try:\n",
    "                        new_sim_image_box = cv2.resize(image,(14,32))\n",
    "                        new_sim_image_box = cv2.cvtColor(new_sim_image_box, cv2.COLOR_BGR2RGB)\n",
    "                        new_sim_images_box.append(new_sim_image_box)\n",
    "                        labels_new_sim.append('Y')\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_sim = []\n",
    "y_new_sim = []\n",
    "\n",
    "# Use dict to store labels\n",
    "y_new_sim = [label_dict[i] for i in labels_new_sim]\n",
    "\n",
    "# Transform them for the model\n",
    "x_new_sim = np.array(new_sim_images_box)\n",
    "y_new_sim = np.array(y_new_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f439b1cdcf8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 143\n",
    "plt.figure()\n",
    "plt.imshow(x_new_sim[i])\n",
    "print(y_new_sim[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 149ms/step\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# load the model we saved\n",
    "model = load_model('model.h5')\n",
    "classification = model.predict_classes(x_new_sim[i].reshape(1, 32, 14, 3), batch_size=10)\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
